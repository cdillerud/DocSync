<analysis>**original_problem_statement:**
The user wants to build a GPI Document Hub to replace their existing Square9 and Zetadocs workflows.

**PRODUCT REQUIREMENTS:**
1.  **Multi-Type Document Classification:** Implement a robust classification system that assigns a , , and  to every ingested document. This system must prioritize deterministic rules and use AI classification (via ) only as a fallback for ambiguous documents.
2.  **Type-Aware Workflow Engine:** The workflow engine must support different state machines based on the document's . Detailed workflows are required for , , , , , and other key types.
3.  **Type-Aware APIs and Queues:** All workflow queue and mutation APIs must be  aware. Generic endpoints are needed for the UI to build dynamic work queues and trigger state transitions.
4.  **Document Type Dashboard:** A frontend view () with a backend API () to display aggregated metrics (counts by status, extraction rates, classification methods) for each , with filtering capabilities.
5.  **CSV Export:** An Export CSV feature for the filtered Document Type Dashboard data.
6.  **Legacy Data Migration:** A backend job to import historical documents from legacy systems (Square9, Zetadocs) into the GPI Hub, correctly classifying them and setting their workflow status to a final, historical state.

**User's preferred language**: English

**what currently exists?**
A full-stack application (FastAPI/React/MongoDB) serving as a multi-document-type hub. Key features include:
- A unified document ingestion pipeline with deterministic-first, AI-assisted classification.
- A type-aware workflow engine in  supporting 10 distinct document types with specific state machines.
- A Document Type Dashboard with advanced filtering (by doc type, source system, and classification method) and detailed metrics, including .
- CSV export functionality for the dashboard.
- A new, but untested, migration module () designed to import legacy documents, complete with a stubbed data source and API endpoints ().

**Last working item**:
- **Last item agent was working:** Implementing a backend migration job to import legacy documents from Square9 and Zetadocs. The agent created the full service structure, including a stubbed data source (), a workflow initializer () to set final states for historical documents, and the main job orchestrator (). API endpoints were also added to  to trigger and monitor the job.
- **Status:** IN PROGRESS
- **Agent Testing Done:** N
- **Which testing method agent to use?** backend testing agent. The agent needs to write unit tests for the components in  and integration tests for the  endpoint to validate both dry run and real run modes.
- **User Testing Done:** N

**All Pending/In progress Issue list**:
- **Issue 1:** Test and validate the legacy document migration job. (P0)

**Issues Detail:**
- **Issue 1:** Test and validate the legacy document migration job.
    - **Attempted fixes:** The agent has completed the full implementation of the migration module, including API endpoints and business logic. Test file stubs have been created under .
    - **Next debug checklist:**
        1.  Write unit tests for , , and .
        2.  Write integration tests for the  endpoint.
        3.  Test the dry run mode to ensure it simulates the migration correctly without writing to the database.
        4.  Test the real run mode to verify that documents are created with the correct , ,  (),  (e.g., , ), and .
        5.  Confirm that migrated documents appear correctly on the dashboard and do not inflate the .
    - **Why fix this issue and what will be achieved with the fix?** This will provide a tested and reliable mechanism to import historical data, a critical step toward decommissioning the legacy systems.
    - **Status:** NOT STARTED
    - **Is recurring issue?** N
    - **Should Test frontend/backend/both after fix?** Backend
    - **Blocked on other issue:** None

**In progress Task List**:
- **Task 1:** Complete the legacy document migration job. (P0)
    - **Where to resume:** Resume by populating the test file  with comprehensive unit and integration tests for the migration service.
    - **What will be achieved with this?** A fully functional and verified migration pipeline for importing historical documents from legacy systems.
    - **Status:** IN PROGRESS
    - **Should Test frontend/backend/both after fix?** Backend
    - **Blocked on something:** None.

**Upcoming and Future Tasks**
**Upcoming Tasks:**
- **Task 1 (P1):** Build a more granular Frontend UI for AP-specific workflow queues.
- **Task 2 (P1):** Implement ingestion from Excel/CSV files for the Sales module.
- **Task 3 (P2):** Define Stable Vendor as a metric based on extraction consistency and volume.
- **Task 4 (P2):** Phase 8 - Controlled Vendor Enablement: Enable automated draft creation in BC for a small subset of stable AP vendors.

**Future Tasks:**
- Wire in real data connectors (e.g., file system, database access) to the migration job, replacing the current stub.
- Flesh out approval routing logic with multi-step approvals.
- Refactor the monolithic  into a more service-oriented structure.
- Implement a Vendor Threshold Override architecture.
- Plan and execute the full decommissioning of the legacy Zetadocs system.
- Implement automated creation of Purchase Invoice *lines* in BC.
- Replace mock JWT auth with Entra ID SSO.

**Completed work in this session**
- **AI-Assisted Classification Pipeline (DONE):** Successfully integrated a deterministic-first, AI-fallback classification mechanism into the document intake process, complete with an audit trail. Fixed a bug preventing the audit data from being saved.
- **Dashboard Enhancement (DONE):** The Document Type Dashboard was enhanced with detailed classification metrics and a UI filter to view documents by classification method (Deterministic, AI Assisted, Other).
- **Expanded Workflow Engine (DONE):** The workflow engine in  was expanded to support 10 document types, each with its own specific state machine. Generic mutation endpoints were added to  to handle state transitions for all types.
- **Migration Job Scaffolding (IN PROGRESS):** Created the complete backend structure and logic for a legacy data migration job.

**Earlier issues found/mentioned but not fixed**
- The technical debt of the monolithic  file remains a future refactoring task.

**Known issue recurrence from previous fork**
- None.

**Code Architecture**


**Key Technical Concepts**
- **Backend:** FastAPI, Pydantic
- **Frontend:** React, Tailwind CSS, Shadcn/UI
- **Database:** MongoDB
- **Workflow Engine:** Type-aware state machine pattern.
- **Classification:** Deterministic-first logic with AI fallback.
- **Data Migration:** Batch processing job with dry-run capability.
- **Deployment:** Docker

**key DB schema**
- ****: , , , , , , , , . The migration will populate fields like , , and .

**changes in tech stack**
- Added  to .

**All files of reference**
- **New Files Created:**
  - : Defines a stubbed legacy data source for the migration job.
  - : Logic to set the final workflow state for migrated docs.
  - : The main migration job orchestrator.
  - : Test file for the new migration feature (currently empty).
  - , , : New test suites for completed features.
- **Key Files Modified:**
  - : Major updates to integrate AI classification, dashboard metrics, generic workflow endpoints, and the new migration APIs.
  - : Major expansion of workflow definitions for all 10 document types.
  - : Updated to display classification metrics and filters.
  - : Updated to reflect completed work.

**Areas that need refactoring**:
-  remains a large monolith. The logic for the dashboard, generic workflows, and migration are candidates to be moved into their own service modules in the future.

**key api endpoints**
- **Migration (NEW):**
  - 
  - 
- **Generic Workflow Mutations (NEW):**
  - 
  - 
  - 
  - , , 
- **Dashboard & Export (UPDATED):**
  -  (now accepts  query param)
  -  (now accepts  query param)

**Critical Info for New Agent**
- **Your immediate task is to test the newly created migration job.** The code is fully implemented in  but is completely untested.
- **Start by writing tests** in the empty file .
- The migration job uses a **stubbed data source** in , which is sufficient for testing the entire pipeline's logic without needing external connections. Focus on verifying the dry run, real run, and the final state of the created documents.

**documents and test reports created in this job**
-  was updated.
- New test suites created at:
  - 
  - 
  - 
  - 

**Last 10 User Messages and any pending HUMAN messages**
The user has been providing sequential feature requests. The most recent ones were:
1.  user: Request to implement the AI-assisted classification pipeline. (Completed)
2.  user: Request to enhance the dashboard with classification metrics and filters. (Completed)
3.  user: Request to expand the workflow engine with detailed state machines for all 10 document types. (Completed)
4.  user: **Request to implement a backend migration job for legacy documents.** (In Progress - code written, pending testing)

**Project Health Check:**
- **Broken:** The new legacy data migration feature is implemented but entirely untested. It should not be considered functional until tests are written and pass.
- **Mocked:**
    - Business Central integration remains in Shadow Mode.
    - The data source for the new legacy migration job is a stub ().

**3rd Party Integrations**
- **Microsoft Graph API:** For email ingestion.
- **Dynamics 365 Business Central API:** For data validation.
- **Gemini (via Emergent LLM Key):** Used for the AI fallback in the document classification pipeline.

**Testing status**
- **Testing agent used after significant changes:** YES, for the classification pipeline and workflow engine expansion.
- **Troubleshoot agent used after agent stuck in loop:** NO
- **Test files created:**
  - 
  - 
  - 
  -  (empty)
- **Known regressions:** None.

**Credentials to test flow:**
-  is available in  for any testing involving the classification pipeline.

**What agent forgot to execute**
The agent correctly identified the next logical step (testing the migration job) and created the necessary file structure before the session ended. Nothing was forgotten.</analysis>
