<analysis>**original_problem_statement:**
The user wants to build a GPI Document Hub to replace their existing Square9 and Zetadocs workflows. The platform must handle multiple document types (AP Invoices, Sales Invoices, Purchase Orders, etc.) coming from various sources.

**PRODUCT REQUIREMENTS:**
1.  **Multi-Type Document Classification:** Implement a robust classification system that assigns a , , and  to every ingested document. This system must prioritize deterministic rules (based on email source, Zetadocs codes, etc.) and only use AI classification (via ) as a fallback when the type is ambiguous (). An audit trail for AI decisions ( field) is required.
2.  **Type-Aware Workflow Engine:** Generalize the existing workflow engine to handle different state machines based on the document's . Implement a full, detailed workflow for  and minimal workflows for other key types (, , etc.).
3.  **Type-Aware APIs and Queues:** All workflow queue and mutation APIs must be  aware. A generic queue endpoint () is needed for the UI to build dynamic work queues.
4.  **Document Type Dashboard:** Create a backend endpoint () and a frontend view () to display aggregated metrics (counts by status, extraction rates, match methods) for each .
5.  **CSV Export:** Add an Export CSV feature to the Document Type Dashboard, allowing users to download the filtered data.

**User's preferred language**: English

**what currently exists?**
A full-stack application (FastAPI/React/MongoDB) that serves as a multi-document-type hub. Key features include:
- A unified document ingestion pipeline.
- Dynamic, UI-driven management of email sources for ingestion.
- A type-aware workflow engine in  that supports different workflows based on .
- Generic and specific API endpoints for workflow queues and metrics, which are  aware.
- A Document Type Dashboard with filters and detailed metrics per document type.
- CSV export functionality for the dashboard data.
- The initial scaffolding for AI-assisted classification, including the  in the environment and a new  file.

**Last working item**:
- **Last item agent was working:** Implementing the deterministic-first AI classification pipeline. The agent created the  service and a new helper function  in . The agent was in the process of integrating this new function into the main document intake logic () to replace the previous, simpler classification method.
- **Status:** IN PROGRESS
- **Agent Testing Done:** N
- **Which testing method agent to use?** backend testing agent. Unit tests should be created for . Integration tests are needed for the  and  endpoints to verify that the deterministic rules are prioritized and the AI fallback is triggered correctly with a proper audit trail.
- **User Testing Done:** N

**All Pending/In progress Issue list**:
- **Issue 1:** Complete and test the AI-assisted document classification pipeline. (P0)

**Issues Detail:**
- **Issue 1:** Complete and test the AI-assisted document classification pipeline.
    - **Attempted fixes:**
        - Created  to encapsulate the Emergent LLM call.
        - Added  to .
        - Added a configurable .
        - Created a new helper function  in  to orchestrate the deterministic-first logic.
        - Started modifying the  function to use this new pipeline.
    - **Next debug checklist:**
        1.  Finalize the integration of  into the  and  handlers in .
        2.  Ensure the  audit field is correctly populated and saved to the database only when the AI classifier is invoked.
        3.  Verify the logic strictly follows the deterministic-first rule: AI is only called if the deterministic classification result is OTHER and the LLM key is present.
        4.  Ensure the confidence threshold is correctly applied before accepting the AI's proposed .
        5.  Write unit tests for the  function in .
        6.  Write integration tests to validate the entire pipeline with both deterministic and ambiguous documents.
    - **Why fix this issue and what will be achieved with the fix?** This will significantly improve the accuracy of document classification for ambiguous documents, which is a critical step in replacing the legacy Square9 system.
    - **Status:** IN PROGRESS
    - **Is recurring issue?** N
    - **Should Test frontend/backend/both after fix?** Backend
    - **Blocked on other issue:** None

**In progress Task List**:
- **Task 1:** Integrate  for safe, AI-assisted document classification. (P0)
    - **Where to resume:** In , inside the  function. The agent needs to replace the old classification logic with a call to the new  helper function and ensure the results (,  audit data) are correctly passed to the database update operation.
    - **What will be achieved with this?** A robust, two-step classification system that improves data accuracy and provides an audit trail for AI-driven decisions.
    - **Status:** IN PROGRESS
    - **Should Test frontend/backend/both after fix?** Backend
    - **Blocked on something:** None.

**Upcoming and Future Tasks**
**Upcoming Tasks:**
- **Task 1 (P0):** Build Frontend UI for AP Workflow Queues. This was planned but superseded by the multi-type dashboard. A more granular queue UI might still be needed.
- **Task 2 (P1):** Implement ingestion from Excel/CSV files for the Sales module.
- **Task 3 (P1):** Define Stable Vendor as a metric based on extraction consistency and volume.
- **Task 4 (P2):** Phase 8 - Controlled Vendor Enablement: Enable automated draft creation in BC for a small subset of stable AP vendors.

**Future Tasks:**
- Flesh out approval routing logic with multi-step approvals.
- Refactor the monolithic  into a more service-oriented structure (e.g., move classification and other logic into dedicated modules).
- Implement a Vendor Threshold Override architecture.
- Plan and execute the full decommissioning of the legacy Zetadocs system.
- Implement automated creation of Purchase Invoice *lines* in BC (Transaction Automation Level 3).
- Replace mock JWT auth with Entra ID SSO.

**Completed work in this session**
- **Type-Aware Workflow Engine:** Refactored the workflow engine to be -aware, supporting different state machines for different document types.
- **Type-Aware APIs:** Implemented generic API endpoints (, ) that filter by .
- **Document Type Dashboard:** Created a new page () and backend API () to provide a comprehensive overview of document metrics, filterable by  and .
- **CSV Export:** Added a feature to export the filtered dashboard data to a CSV file via a new  endpoint.
- **AI Classification Foundation:** Began implementing the deterministic-first AI classification pipeline.

**Earlier issues found/mentioned but not fixed**
- The technical debt of the monolithic  file remains a future refactoring task.

**Known issue recurrence from previous fork**
- None. The previous recurring issue (Automatic workflow for AP documents not being triggered) was resolved by the implementation of the new, robust, type-aware workflow engine.

**Code Architecture**


**Key Technical Concepts**
- **Backend:** FastAPI, Pydantic
- **Frontend:** React, Tailwind CSS, Shadcn/UI
- **Database:** MongoDB
- **Workflow Engine:** Type-aware state machine pattern.
- **Classification:** Deterministic-first logic, with AI classification as a fallback.
- **Deployment:** Docker

**key DB schema**
- ****: , , , , , , ,  (NEW, conditional).

**changes in tech stack**
- None.

**All files of reference**
- : Major modifications to integrate the type-aware workflow, dashboard APIs, and AI classification pipeline.
- : Rewritten to be type-aware.
- : New file for handling LLM-based classification.
- : New file for the dashboard UI.
- : Updated with functions for the dashboard and export APIs.
- : Updated with .

**Areas that need refactoring**:
-  remains a large monolith. The new classification pipeline logic () is a prime candidate to be moved into a separate service/module to further clean up .

**key api endpoints**
- **Dashboard & Export (NEW):**
    - 
    - 
- **Generic Workflow (NEW):**
    - 
    - 
- **Document Intake:**
    - 
    - 

**Critical Info for New Agent**
- **Classification is Deterministic-First:** You MUST complete the implementation ensuring that deterministic rules (from source system, Zetadocs codes, etc.) are always prioritized. The AI classifier in  should only be called as a fallback when the initial classification is .
- **Resume in :** The immediate task is to finish wiring the  function into the  function, ensuring the database update includes both the final  and the conditional  audit data.
- **Test Thoroughly:** After implementation, it is critical to test the entire classification pipeline to ensure both deterministic and AI-driven paths work as expected.

**documents and test reports created in this job**
- : Has been updated with the latest features, including the multi-type workflow and dashboard.

**Last 10 User Messages and any pending HUMAN messages**
1.  user: Requests a Document Type Dashboard with aggregated metrics.
2.  agent: Implements the backend API and frontend UI for the dashboard.
3.  user: Verifies the dashboard and requests CSV export functionality.
4.  agent: Implements the CSV export endpoint and adds a button to the UI.
5.  user: Verifies the export and provides a detailed specification for integrating the  for AI-assisted classification. The core requirement is a deterministic-first approach where AI is only used as a fallback.
6.  agent: Acknowledges the new requirement and begins implementation by creating the AI classifier service and modifying . (This is the current state).

**Project Health Check:**
- **Broken:** The document classification pipeline is mid-refactor. Documents are ingested but the new AI-assisted logic is not fully wired in.
- **Mocked:** Business Central integration remains in Shadow Mode.

**3rd Party Integrations**
- **Microsoft Graph API:** For email ingestion.
- **Dynamics 365 Business Central API:** For data validation.
- **Gemini (via Emergent LLM Key):** Used for initial document classification, now being integrated into a more robust, deterministic-first pipeline.

**Testing status**
- **Testing agent used after significant changes:** YES (for the dashboard and export features).
- **Troubleshoot agent used after agent stuck in loop:** NO
- **Test files created:** 
- **Known regressions:** None.

**Credentials to test flow:**
-  is available in the  file for testing the AI classification feature.

**What agent forgot to execute**
- The agent is in the middle of a complex task and has not forgotten anything. It correctly identified the next steps to complete the AI classification integration.</analysis>
